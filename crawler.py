import os
import time
import requests
from urllib.parse import urlparse, unquote
from bs4 import BeautifulSoup
from tqdm import tqdm

# â”€â”€â”€ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_URL = "https://scholar.google.co.kr/scholar"
HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/114.0.0.0 Safari/537.36"
    )
}
KEYWORDS = [
    "Respiratory Infections", "Diarrheal Diseases",
    "Trauma Care", "Burn Treatment"
]
""""Measles", "Cholera", "Hepatitis A", "Malaria",   
    "Acute Watery Diarrhoea", "Diphtheria"""
OUTPUT_DIR = "downloaded_pdfs"

# â˜… ëª©í‘œ ë‹¤ìš´ë¡œë“œ ì„±ê³µ ê°œìˆ˜
MAX_DOWNLOADS = 50

# â˜… í‚¤ì›Œë“œë‹¹ ì—¬ìœ  ìˆê²Œ ìˆ˜ì§‘í•  ë§í¬ ìˆ˜
MAX_PER_KEYWORD = 15     

MAX_PAGES_PER_KEYWORD = 20
DELAY_BETWEEN_PAGES = 10
MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MBë¡œ ìƒí–¥

os.makedirs(OUTPUT_DIR, exist_ok=True)

def fetch_pdf_links_one_page(query: str, page: int):
    params = {
        "q": f"{query} treatment",
        "hl": "ko",
        "as_ylo": 2025,
        "as_sdt": "0,5",
        "start": page * 10
    }
    resp = requests.get(BASE_URL, params=params, headers=HEADERS)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "html.parser")
    urls = []
    for span in soup.find_all("span", class_="gs_ctg2"):
        if span.get_text(strip=True) == "[PDF]":
            a = span.find_parent("a", href=True)
            urls.append(a["href"])
    return urls

def collect_pdf_links(keywords):
    """
    í‚¤ì›Œë“œë³„ë¡œ MAX_PER_KEYWORD ë§í¬ë¥¼ ëª¨ì•„ì„œ
    ì´ ìµœëŒ€ len(keywords) * MAX_PER_KEYWORD ê°œ ë§í¬ ë°˜í™˜
    """
    collected = []
    seen = set()

    for kw in keywords:
        print(f"\nâ–¶ '{kw}' ê²€ìƒ‰ ì‹œì‘ (ìµœëŒ€ {MAX_PER_KEYWORD}ê°œ)")
        per_count = 0

        for page in range(MAX_PAGES_PER_KEYWORD):
            if per_count >= MAX_PER_KEYWORD:
                break

            print(f"  â€¢ í˜ì´ì§€ {page+1}", end="â€¦ ")
            try:
                urls = fetch_pdf_links_one_page(kw, page)
            except Exception as e:
                print(f"ì‹¤íŒ¨({e})")
                break

            added = 0
            for url in urls:
                if per_count >= MAX_PER_KEYWORD:
                    break
                if url not in seen:
                    seen.add(url)
                    collected.append(url)
                    per_count += 1
                    added += 1

            print(f"í‚¤ì›Œë“œ '{kw}' ìˆ˜ì§‘ {per_count}ê°œ (ì´ë²ˆ í˜ì´ì§€ +{added})")
            if per_count >= MAX_PER_KEYWORD:
                print(f"  â€¢ '{kw}' í‚¤ì›Œë“œ ìˆ˜ì§‘ ì™„ë£Œ\n")
                break

            time.sleep(DELAY_BETWEEN_PAGES)

    print(f"\nâ–¶ ì´ {len(collected)}ê°œ ë§í¬ ìˆ˜ì§‘ ì™„ë£Œ (ë§í¬ í’€ë§ ì™„ë£Œ)")
    return collected

def download_pdfs(urls, output_dir):
    """
    ë§í¬ í’€ì—ì„œ ì„±ê³µì ìœ¼ë¡œ 100ê°œ ë‹¤ìš´ë¡œë“œë  ë•Œê¹Œì§€ ì‹œë„
    skip 403, ëŒ€ìš©ëŸ‰ íŒŒì¼, ì‹¤íŒ¨ ë“±ì€ ê±´ë„ˆëœ€
    """
    session = requests.Session()
    success_count = 0

    for idx, url in enumerate(tqdm(urls, desc="ë‹¤ìš´ë¡œë“œ ì§„í–‰"), start=1):
        if success_count >= MAX_DOWNLOADS:
            break

        # 1) HEAD ì²´í¬
        try:
            head = session.head(url, headers=HEADERS, timeout=5)
            if head.status_code == 403 or head.status_code != 200:
                continue
            size = int(head.headers.get("Content-Length", 0) or 0)
            if size > MAX_FILE_SIZE:
                continue
        except:
            continue

        # 2) íŒŒì¼ëª… ê²°ì •
        path = urlparse(url).path
        fname = unquote(os.path.basename(path).split("?")[0])
        if not fname.lower().endswith(".pdf"):
            fname = f"{idx}_0526_2.pdf"
        dest = os.path.join(output_dir, fname)
        if os.path.exists(dest):
            continue  # ì´ë¯¸ ìˆìœ¼ë©´ skip

        # 3) ìŠ¤íŠ¸ë¦¬ë° ë‹¤ìš´ë¡œë“œ
        try:
            with session.get(url, headers=HEADERS, stream=True, timeout=(5,10)) as r:
                r.raise_for_status()
                with open(dest, "wb") as f:
                    for chunk in r.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
            success_count += 1
        except Exception:
            continue

    print(f"\nâœ… ì´ {success_count}ê°œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì„±ê³µ")

if __name__ == "__main__":
    # 1) ë§í¬ ìˆ˜ì§‘
    all_links = collect_pdf_links(KEYWORDS)

    # 2) ë‹¤ìš´ë¡œë“œ (100ê°œ ì±„ìš¸ ë•Œê¹Œì§€)
    download_pdfs(all_links, OUTPUT_DIR)

    print("\nğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ")
