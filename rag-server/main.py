import asyncio
import json
from typing import List, AsyncGenerator, Dict, Optional
from contextlib import asynccontextmanager
from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException
from fastapi.responses import HTMLResponse
from langchain_pinecone import PineconeVectorStore
from langchain_upstage import UpstageEmbeddings, ChatUpstage
from langchain.chains import RetrievalQA
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
import uvicorn
from config import config
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ConnectionManager:
    def __init__(self):
        self.active_connections: Dict[str, WebSocket] = {}

    async def connect(self, websocket: WebSocket, client_id: str = None):
        # í•­ìƒ WebSocketì„ accept
        await websocket.accept()
            
        if client_id:
            self.active_connections[client_id] = websocket
            logger.info(f"í´ë¼ì´ì–¸íŠ¸ {client_id} ì—°ê²°ë¨")
        else:
            # client_idê°€ ì—†ëŠ” ê²½ìš° websocket ê°ì²´ë¥¼ í‚¤ë¡œ ì‚¬ìš©
            self.active_connections[id(websocket)] = websocket

    def update_client_id(self, websocket: WebSocket, old_key: any, new_client_id: str):
        """ê¸°ì¡´ ì—°ê²°ì˜ í´ë¼ì´ì–¸íŠ¸ IDë¥¼ ì—…ë°ì´íŠ¸"""
        if old_key in self.active_connections:
            del self.active_connections[old_key]
        self.active_connections[new_client_id] = websocket
        logger.info(f"í´ë¼ì´ì–¸íŠ¸ ID ì—…ë°ì´íŠ¸: {old_key} -> {new_client_id}")

    def disconnect(self, websocket: WebSocket, client_id: str = None):
        if client_id and client_id in self.active_connections:
            del self.active_connections[client_id]
            logger.info(f"í´ë¼ì´ì–¸íŠ¸ {client_id} ì—°ê²° í•´ì œë¨")
        else:
            # client_idê°€ ì—†ëŠ” ê²½ìš° websocket ê°ì²´ë¡œ ì°¾ì•„ì„œ ì œê±°
            ws_id = id(websocket)
            if ws_id in self.active_connections:
                del self.active_connections[ws_id]

    async def send_personal_message(self, message: str, client_id: str):
        if client_id in self.active_connections:
            websocket = self.active_connections[client_id]
            try:
                await websocket.send_text(message)
            except Exception as e:
                logger.error(f"ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨ - í´ë¼ì´ì–¸íŠ¸ {client_id}: {e}")
                self.disconnect(websocket, client_id)

manager = ConnectionManager()

class LLMService:
    def __init__(self):
        self.client = None
        self.vectorstore = None
        self.embeddings = None
        self.base_retriever = None
        self.reranker_retriever = None
        self.rag_chain = None
        self.advanced_rag_chain = None
        self._initialized = False
        
    async def initialize(self):
        """ë¹„ë™ê¸° ì´ˆê¸°í™”"""
        if self._initialized:
            return
            
        try:
            # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
            config.setup_environment()
            
            # Upstage í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
            self.client = ChatUpstage(
                api_key=config.get_upstage_api_key(),
                model=config.UPSTAGE_MODEL,
                streaming=True,
                temperature=0
            )
            
            # ì„ë² ë”© ë° ë²¡í„°ìŠ¤í† ì–´ ì„¤ì •
            self.embeddings = UpstageEmbeddings(model=config.EMBEDDING_MODEL)
            self.vectorstore = PineconeVectorStore(
                index_name=config.PINECONE_INDEX_NAME,
                embedding=self.embeddings
            )
            
            # ê¸°ë³¸ ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì •
            self.base_retriever = self.vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={"k": config.RETRIEVAL_K}
            )

            # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
            await self._setup_reranker()
            self._setup_prompts()
            self._setup_chains()
            
            self._initialized = True
            logger.info("LLMService ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"LLMService ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise HTTPException(status_code=500, detail=f"ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        
    async def close(self):
        """ëª¨ë“  ë¦¬ì†ŒìŠ¤ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤."""
        try:
            if self.vectorstore:
                # Pinecone ì—°ê²° ì •ë¦¬ (í•„ìš”í•œ ê²½ìš°)
                pass
            if self.client:
                # Upstage í´ë¼ì´ì–¸íŠ¸ ì •ë¦¬ (í•„ìš”í•œ ê²½ìš°)
                pass
            logger.info("LLMService ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        
    async def _setup_reranker(self):
        """Cross-encoder reranker ì„¤ì •"""
        try:
            # Cross-encoder ëª¨ë¸ ì´ˆê¸°í™”
            cross_encoder = HuggingFaceCrossEncoder(
                model_name="cross-encoder/ms-marco-MiniLM-L-6-v2"
            )
            
            # Reranker ì»´í”„ë ˆì„œ ì„¤ì •
            self.reranker_compressor = CrossEncoderReranker(model=cross_encoder)
            
            # Contextual Compression Retrieverë¡œ reranker ì ìš©
            self.reranker_retriever = ContextualCompressionRetriever(
                base_compressor=self.reranker_compressor,
                base_retriever=self.base_retriever
            )
            logger.info("Reranker ì„¤ì • ì™„ë£Œ")

        except Exception as e:
            logger.error(f"Reranker ì„¤ì • ì‹¤íŒ¨: {e}")
            # Reranker ì„¤ì • ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ retriever ì‚¬ìš© 
            self.reranker_retriever = self.base_retriever  
    
    def _setup_prompts(self):
        """í”„ë¡¬í”„íŠ¸ ì„¤ì •"""
        self.rag_prompt = PromptTemplate(
            template="""Role Definition:
You are a specialized AI assistant for medical literature analysis. Provide accurate and comprehensive answers based solely on the provided research paper context, with particular emphasis on experimental data and graphical evidence.
ë˜í•œ ë‹µë³€ì€ ì›¬ë§Œí•˜ë©´ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì¤˜

Core Instructions:
1. Fundamental Principles
- Base all responses strictly on the provided paper context
- If information is uncertain or unavailable, clearly state "I don't know" or "This information is not available in the provided context"
- Never speculate or extrapolate beyond the given evidence, as medical accuracy is critical

2. Response Structure Template
Format your responses as follows:

## Direct Answer
[Clear, direct response to the question]

## Experimental Evidence Analysis
### Research Objectives
[Study purpose, hypothesis, and design rationale]

### Key Experimental Findings
[Methodology, results from graphs/data, statistical outcomes]

### Clinical Significance
[Therapeutic implications, statistical significance, clinical applications]

## Important Considerations
[Limitations, contraindications, safety considerations]

IMPORTANT: If asked about information not contained in the provided paper, respond: "This information is not available in the provided research paper. I can only analyze and discuss findings explicitly presented in the given context."

Question: {input}

Context: {context}

Answer:""",
            input_variables=["input", "context"]
        )

        self.classification_prompt = PromptTemplate(
            template="""ë‹¤ìŒ ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ì„¸ìš”. ë°˜ë“œì‹œ ë§ˆì§€ë§‰ ì¤„ì— ê²°ê³¼ë§Œ ì¶œë ¥í•˜ì„¸ìš”.

ë¶„ë¥˜ ê¸°ì¤€:
- RAG_NEEDED: ìµœì‹  ì˜ë£Œ ì •ë³´ë‚˜ ë¬¸ì„œ ê²€ìƒ‰ì´ í•„ìš”í•œ ì§ˆë¬¸ (ì˜ë£Œ ì—°êµ¬, ì¹˜ë£Œë²•, ì•½ë¬¼, ì§ˆë³‘ ì •ë³´ ë“±)
- GENERAL_CHAT: ì¼ë°˜ì ì¸ ëŒ€í™”ë‚˜ ê¸°ë³¸ ì§€ì‹ ì§ˆë¬¸ (ì¸ì‚¬, ê°„ë‹¨í•œ ì„¤ëª…, ì¼ë°˜ ìƒì‹ ë“±)

ì§ˆë¬¸: {question}

ë¶„ë¥˜ ê²°ê³¼: [RAG_NEEDED ë˜ëŠ” GENERAL_CHAT]""",
            input_variables=["question"]
        )
        
    def _setup_chains(self):
        """RAG ì²´ì¸ ì„¤ì • (Reranker í¬í•¨)"""
        def format_docs(docs):
            """ë¬¸ì„œë“¤ì„ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ë¡œ í¬ë§·íŒ…"""
            if not docs:
                return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            top_docs = docs[:5]
            formatted_docs = []
            for i, doc in enumerate(top_docs, 1):
                # reranker scoreê°€ ìˆë‹¤ë©´ í‘œì‹œ
                score_info = ""
                if hasattr(doc, 'metadata') and 'relevance_score' in doc.metadata:
                    score_info = f" (ê´€ë ¨ë„: {doc.metadata['relevance_score']:.3f})"
                
                formatted_docs.append(f"[ë¬¸ì„œ {i}{score_info}]\n{doc.page_content}")
            
            return "\n\n".join(formatted_docs)

        # ê¸°ë³¸ RAG ì²´ì¸
        self.rag_chain = (
            {
                "context": self.reranker_retriever | format_docs,
                "input": RunnablePassthrough()
            }
            | self.rag_prompt
            | self.client
            | StrOutputParser()
        )
        
        # ê³ ê¸‰ ì²´ì¸ êµ¬ì¡° (ì†ŒìŠ¤ ë¬¸ì„œ ì •ë³´ë„ í•¨ê»˜ ë°˜í™˜)
        self.advanced_rag_chain = (
            {
                "source": self.reranker_retriever,
                "input": RunnablePassthrough()
            }
            | RunnablePassthrough.assign(
                answer = {
                    "context": lambda x: format_docs(x['source']),
                    "input": lambda x: x["input"]
                }
                | self.rag_prompt
                | self.client
                | StrOutputParser()
            )
        )
            
    async def should_use_rag(self, question: str) -> bool:
        """RAG ì‚¬ìš© ì—¬ë¶€ íŒë‹¨"""
        try:
            if not self._initialized:
                await self.initialize()
                
            formatted_prompt = self.classification_prompt.format(question=question)
            classification_result = await self.client.ainvoke([HumanMessage(content=formatted_prompt)])
            lines = classification_result.content.strip().split('\n')
            last_line = lines[-1]
            
            return "RAG_NEEDED" in last_line
        
        except Exception as e:
            logger.error(f"ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            return False

    async def get_streaming_answer(self, question: str, websocket: WebSocket, client_id: str = None) -> str:
        """WebSocketì„ í†µí•œ ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„±"""
        try:
            if not self._initialized:
                await self.initialize()
                
            # RAG í•„ìš”ì„± íŒë‹¨
            needs_rag = await self.should_use_rag(question)
            
            if needs_rag:
                # RAGë¥¼ ì‚¬ìš©í•œ ë‹µë³€
                await self._send_websocket_message(websocket, client_id, "ì˜ë£Œ ë¬¸í—Œì„ ê²€ìƒ‰í•˜ì—¬ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤...\n\n", "token")
                
                full_answer = ""
                async for chunk in self.stream_rag_response(question, use_advanced_chain=False):
                    if chunk:
                        await self._send_websocket_message(websocket, client_id, chunk, "token")
                        full_answer += chunk
                
                # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ ì‹ í˜¸
                await self._send_websocket_message(websocket, client_id, full_answer, "stream_end")
                return full_answer
            else:
                # ì¼ë°˜ ë‹µë³€
                await self._send_websocket_message(websocket, client_id, "ë‹µë³€ì„ ìƒì„±í•˜ê² ìŠµë‹ˆë‹¤...\n\n", "token")
                
                full_answer = ""
                async for chunk in self.stream_response(question):
                    if chunk:
                        await self._send_websocket_message(websocket, client_id, chunk, "token")
                        full_answer += chunk
                
                # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ ì‹ í˜¸
                await self._send_websocket_message(websocket, client_id, full_answer, "stream_end")
                return full_answer
                
        except Exception as e:
            error_msg = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            await self._send_websocket_message(websocket, client_id, error_msg, "error")
            return error_msg

    async def _send_websocket_message(self, websocket: WebSocket, client_id: str, content: str, msg_type: str):
        """WebSocket ë©”ì‹œì§€ ì „ì†¡ í—¬í¼ í•¨ìˆ˜"""
        try:
            response = {
                "type": msg_type,
                "content": content
            }
            
            if client_id:
                response["clientId"] = client_id
                
            await websocket.send_text(json.dumps(response))
        except Exception as e:
            logger.error(f"WebSocket ë©”ì‹œì§€ ì „ì†¡ ì˜¤ë¥˜: {e}")
            # ì—°ê²°ì´ ëŠì–´ì§„ ê²½ìš° managerì—ì„œ ì œê±°
            manager.disconnect(websocket, client_id)

    async def stream_response(self, question: str) -> AsyncGenerator[str, None]:
        """ì¼ë°˜ì ì¸ ì§ˆë¬¸ì— ëŒ€í•œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ"""
        try:
            # ë‹¨ìˆœíˆ HumanMessage í•˜ë‚˜ë§Œ ìƒì„±
            message = HumanMessage(content=question)
            async for chunk in self.client.astream([message]):
                if chunk.content:
                    yield chunk.content
        except Exception as e:
            logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            yield f"Error: {str(e)}"
    
    async def stream_rag_response(self, question: str, use_advanced_chain: bool = False) -> AsyncGenerator[str, None]:
        """RAG ì²´ì¸ì„ ì‚¬ìš©í•œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ"""
        try:
            chain = self.advanced_rag_chain if use_advanced_chain else self.rag_chain
            
            if use_advanced_chain:
                # ê³ ê¸‰ ì²´ì¸ì˜ ê²½ìš° ê²°ê³¼ì—ì„œ answer ë¶€ë¶„ë§Œ ìŠ¤íŠ¸ë¦¬ë°
                async for result in chain.astream(question):
                    if isinstance(result, dict) and 'answer' in result:
                        yield result['answer']
                    elif isinstance(result, str):
                        yield result
            else:
                # ê¸°ë³¸ ì²´ì¸
                async for chunk in chain.astream(question):
                    if chunk:
                        yield chunk
                        
        except Exception as e:
            logger.error(f"RAG ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            yield f"RAG ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"



# ì „ì—­ LLMService ì¸ìŠ¤í„´ìŠ¤
llm_service = LLMService()

@asynccontextmanager
async def lifespan(app: FastAPI):
    # ì‹œì‘ ì‹œ
    try:
        await llm_service.initialize()
        logger.info("ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì™„ë£Œ")
    except Exception as e:
        logger.error(f"ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹¤íŒ¨: {e}")
        raise
    
    yield
    
    # ì¢…ë£Œ ì‹œ
    await llm_service.close()
    logger.info("ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œ ì™„ë£Œ")

# FastAPI ì•± ìƒì„± (ìˆ˜ì •ëœ lifespan ì‚¬ìš©)
app = FastAPI(
    title="LangChain WebSocket QA Server",
    lifespan=lifespan
)

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    client_id = None
    websocket_id = id(websocket)
    connected = False
    
    try:
        # ì´ˆê¸° ì—°ê²° (client_id ì—†ì´)
        await websocket.accept()
        connected = True
        manager.active_connections[websocket_id] = websocket
        logger.info(f"WebSocket ì—°ê²°ë¨: {websocket_id}")
        
        while True:
            data = await websocket.receive_text()
            message_data = json.loads(data)
            
            # chat.jsì—ì„œ ë³´ë‚´ëŠ” í˜•ì‹ì— ë§ì¶° ì²˜ë¦¬
            if message_data.get("type") == "question":
                question = message_data["content"]
                new_client_id = message_data.get("clientId")  # clientId ì¶”ì¶œ
                
                logger.info(f"ì§ˆë¬¸ ë°›ìŒ - í´ë¼ì´ì–¸íŠ¸ ID: {new_client_id}, ì§ˆë¬¸: {question}")
                
                # í´ë¼ì´ì–¸íŠ¸ IDê°€ ì²˜ìŒ ì œê³µë˜ê±°ë‚˜ ë³€ê²½ëœ ê²½ìš° ì—…ë°ì´íŠ¸
                if new_client_id and new_client_id != client_id:
                    old_key = client_id if client_id else websocket_id
                    manager.update_client_id(websocket, old_key, new_client_id)
                    client_id = new_client_id
                
                # LLMServiceë¥¼ ì‚¬ìš©í•œ ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„±
                await llm_service.get_streaming_answer(question, websocket, client_id)
            else:
                # ê¸°ì¡´ ë°©ì‹ í˜¸í™˜ì„± ìœ ì§€
                question = message_data.get("content", "")
                await llm_service.get_streaming_answer(question, websocket)
                
    except WebSocketDisconnect:
        logger.info(f"WebSocket ì—°ê²° ëŠì–´ì§ - í´ë¼ì´ì–¸íŠ¸: {client_id}")
    except json.JSONDecodeError as e:
        logger.error(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        if connected:
            try:
                await websocket.close(code=1003, reason="Invalid JSON")
            except:
                pass
    except Exception as e:
        logger.error(f"WebSocket ì˜¤ë¥˜: {e}")
        if connected:
            try:
                await websocket.close(code=1011, reason="Internal server error")
            except:
                pass
    finally:
        # ì—°ê²° ì •ë¦¬
        if client_id:
            manager.disconnect(websocket, client_id)
        elif websocket_id in manager.active_connections:
            del manager.active_connections[websocket_id]
            logger.info(f"WebSocket ì—°ê²° ì •ë¦¬ë¨: {websocket_id}")

@app.get("/health")
async def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        # ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ìƒíƒœ í™•ì¸
        if not llm_service._initialized:
            return {
                "status": "initializing", 
                "message": "LLMServiceê°€ ì´ˆê¸°í™” ì¤‘ì…ë‹ˆë‹¤."
            }
        
        return {
            "status": "healthy", 
            "message": "LLMService ìŠ¤íŠ¸ë¦¬ë° ì„œë²„ê°€ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤.",
            "active_connections": len(manager.active_connections)
        }
    except Exception as e:
        logger.error(f"Health check ì‹¤íŒ¨: {e}")
        return {
            "status": "unhealthy", 
            "message": f"ì„œë²„ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {str(e)}"
        }

if __name__ == "__main__":
    print("ğŸš€ LangChain LLMService ìŠ¤íŠ¸ë¦¬ë° WebSocket ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    print(f"ğŸ“ ì„œë²„ ì£¼ì†Œ: http://{config.HOST}:{config.PORT}")
    print(f"ğŸ”Œ WebSocket ì—”ë“œí¬ì¸íŠ¸: ws://{config.HOST}:{config.PORT}/ws")
    print(f"ğŸ¥ Health Check: http://{config.HOST}:{config.PORT}/health")
    print("âœ¨ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„±ì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
    print("ğŸ”„ RAG ë° Reranker ê¸°ëŠ¥ì´ í¬í•¨ë˜ì—ˆìŠµë‹ˆë‹¤!")
    uvicorn.run(app, host=config.HOST, port=config.PORT)
